'use client';

import { useState, useRef, useCallback, useEffect } from 'react';
import * as Tone from 'tone';
import { PitchDetector } from 'pitchy';

// AudioSystemPhase enum (from /test/separated-audio/)
export enum AudioSystemPhase {
  IDLE = 'idle',
  TRANSITIONING = 'transitioning',
  BASE_TONE_PHASE = 'base_tone',
  SCORING_PHASE = 'scoring',
  ERROR_STATE = 'error'
}

// å€éŸ³è£œæ­£è¨­å®šã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆ/test/separated-audio/ç§»æ¤ï¼‰
export interface HarmonicCorrectionConfig {
  fundamentalSearchRange: number;    // åŸºéŸ³æ¢ç´¢ç¯„å›²ï¼ˆÂ±50Hzï¼‰
  harmonicRatios: number[];          // å€éŸ³æ¯”ç‡ [0.5, 2.0, 3.0, 4.0]
  confidenceThreshold: number;       // ç¢ºä¿¡åº¦ã—ãã„å€¤ï¼ˆ0.8ï¼‰
  stabilityBuffer: number[];         // å®‰å®šåŒ–ãƒãƒƒãƒ•ã‚¡ï¼ˆéå»5ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰
  vocalRange: { min: number, max: number }; // äººé–“éŸ³åŸŸï¼ˆ130-1047Hz, C3-C6ï¼‰
}

// AudioEngineè¨­å®šã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
export interface AudioEngineConfig {
  mode: 'random' | 'continuous' | 'chromatic';
  enablePitchDetection: boolean;
  enableHarmonicCorrection: boolean;
  baseNotes: string[];
  harmonicConfig?: Partial<HarmonicCorrectionConfig>;
}

// AudioEngineæˆ»ã‚Šå€¤ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
export interface AudioEngineReturn {
  // åŸºéŸ³å†ç”Ÿæ©Ÿèƒ½
  playBaseTone: (note: string) => Promise<void>;
  stopBaseTone: () => void;
  
  // ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³æ©Ÿèƒ½
  startPitchDetection: () => Promise<void>;
  stopPitchDetection: () => void;
  
  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡º
  currentPitch: number | null;
  correctedPitch: number | null;
  confidence: number;
  
  // çŠ¶æ…‹ç®¡ç†
  isPlaying: boolean;
  phase: AudioSystemPhase;
  error: string | null;
}

// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€éŸ³è£œæ­£è¨­å®šï¼ˆ/test/separated-audio/æº–æ‹ ï¼‰
const DEFAULT_HARMONIC_CONFIG: HarmonicCorrectionConfig = {
  fundamentalSearchRange: 50,
  harmonicRatios: [0.5, 2.0, 3.0, 4.0],  // 1/2å€éŸ³, 2å€éŸ³, 3å€éŸ³, 4å€éŸ³
  confidenceThreshold: 0.8,
  stabilityBuffer: [],
  vocalRange: { min: 130.81, max: 1046.50 } // C3-C6
};

// ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
const DEFAULT_CONFIG: AudioEngineConfig = {
  mode: 'random',
  enablePitchDetection: false,
  enableHarmonicCorrection: false,
  baseNotes: ['C4', 'D4', 'E4', 'F4', 'G4', 'A4', 'B4', 'C5', 'D5', 'E5']
};

/**
 * useAudioEngine Hook
 * éŸ³éŸ¿ã‚·ã‚¹ãƒ†ãƒ çµ±åˆHook - Tone.js + Pitchy + å€éŸ³è£œæ­£ã®çµ±åˆç®¡ç†
 */
export function useAudioEngine(config: Partial<AudioEngineConfig> = {}): AudioEngineReturn {
  // è¨­å®šã®çµ±åˆ
  const mergedConfig = { ...DEFAULT_CONFIG, ...config };
  const harmonicConfig = { ...DEFAULT_HARMONIC_CONFIG, ...config.harmonicConfig };
  
  // åŸºæœ¬çŠ¶æ…‹ç®¡ç†
  const [isPlaying, setIsPlaying] = useState(false);
  const [phase, setPhase] = useState<AudioSystemPhase>(AudioSystemPhase.IDLE);
  const [error, setError] = useState<string | null>(null);
  
  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡ºãƒ‡ãƒ¼ã‚¿
  const [currentPitch, setCurrentPitch] = useState<number | null>(null);
  const [correctedPitch, setCorrectedPitch] = useState<number | null>(null);
  const [confidence, setConfidence] = useState<number>(0);
  
  // Refç®¡ç†
  const configRef = useRef(mergedConfig);
  const harmonicConfigRef = useRef(harmonicConfig);
  const samplerRef = useRef<Tone.Sampler | null>(null);
  const isInitializedRef = useRef(false);
  
  // Pitchyçµ±åˆç”¨ã®Ref
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const pitchDetectorRef = useRef<PitchDetector<Float32Array> | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const [isMicInitialized, setIsMicInitialized] = useState(false);
  
  // å€éŸ³è£œæ­£ã‚·ã‚¹ãƒ†ãƒ ç”¨ã®Refï¼ˆ/test/separated-audio/ç§»æ¤ï¼‰
  const previousFrequencyRef = useRef<number | null>(null);
  const frequencyHistoryRef = useRef<number[]>([]);
  const stabilityBufferRef = useRef<number[]>([]);
  
  // è¨­å®šæ›´æ–°
  configRef.current = mergedConfig;
  harmonicConfigRef.current = harmonicConfig;
  
  // ã‚¨ãƒ©ãƒ¼å‡¦ç†
  const handleError = useCallback((errorMessage: string) => {
    console.error('[useAudioEngine]', errorMessage);
    setError(errorMessage);
    setPhase(AudioSystemPhase.ERROR_STATE);
  }, []);
  
  // ã‚¨ãƒ©ãƒ¼ã‚¯ãƒªã‚¢
  const clearError = useCallback(() => {
    setError(null);
    if (phase === AudioSystemPhase.ERROR_STATE) {
      setPhase(AudioSystemPhase.IDLE);
    }
  }, [phase]);

  // Tone.js Salamander PianoåˆæœŸåŒ–
  const initializeSampler = useCallback(async (): Promise<void> => {
    try {
      if (isInitializedRef.current && samplerRef.current) {
        return; // æ—¢ã«åˆæœŸåŒ–æ¸ˆã¿
      }

      console.log('[useAudioEngine] Salamander PianoåˆæœŸåŒ–é–‹å§‹');
      
      // AudioContexté–‹å§‹
      if (Tone.getContext().state !== 'running') {
        await Tone.start();
        console.log('[useAudioEngine] AudioContexté–‹å§‹å®Œäº†');
      }

      // Salamander Pianoè¨­å®šï¼ˆæ—¢å­˜3ãƒ¢ãƒ¼ãƒ‰äº’æ›ï¼‰
      const sampler = new Tone.Sampler({
        urls: {
          "C4": "C4.mp3"
        },
        baseUrl: "https://tonejs.github.io/audio/salamander/",
        release: 1.5,         // æ—¢å­˜çµ±ä¸€è¨­å®š
        volume: 6             // iPhoneæœ€é©åŒ–è¨­å®š
      }).toDestination();

      // éŸ³æºèª­ã¿è¾¼ã¿å¾…æ©Ÿ
      console.log('[useAudioEngine] ãƒ”ã‚¢ãƒéŸ³æºèª­ã¿è¾¼ã¿ä¸­...');
      await Tone.loaded();
      console.log('[useAudioEngine] ãƒ”ã‚¢ãƒéŸ³æºèª­ã¿è¾¼ã¿å®Œäº†');

      samplerRef.current = sampler;
      isInitializedRef.current = true;
      
    } catch (err) {
      handleError(`Salamander PianoåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: ${err}`);
      throw err;
    }
  }, [handleError]);

  // samplerç ´æ£„ãƒ»ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾
  const disposeSampler = useCallback(() => {
    try {
      if (samplerRef.current) {
        console.log('[useAudioEngine] samplerç ´æ£„');
        samplerRef.current.dispose();
        samplerRef.current = null;
      }
      isInitializedRef.current = false;
    } catch (err) {
      console.error('[useAudioEngine] samplerç ´æ£„ã‚¨ãƒ©ãƒ¼:', err);
    }
  }, []);

  // ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ãƒ»Web Audio APIåˆæœŸåŒ–
  const initializeMicrophone = useCallback(async (): Promise<void> => {
    try {
      if (!configRef.current.enablePitchDetection) {
        console.log('[useAudioEngine] éŸ³ç¨‹æ¤œå‡ºç„¡åŠ¹ï¼ˆè¨­å®šã«ã‚ˆã‚Šï¼‰');
        return;
      }

      if (isMicInitialized && audioContextRef.current && pitchDetectorRef.current) {
        console.log('[useAudioEngine] ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³æ—¢ã«åˆæœŸåŒ–æ¸ˆã¿');
        return;
      }

      console.log('[useAudioEngine] ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹');

      // ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ã‚¢ã‚¯ã‚»ã‚¹è¦æ±‚ï¼ˆ/test/separated-audio/è¨­å®šæº–æ‹ ï¼‰
      streamRef.current = await navigator.mediaDevices.getUserMedia({
        audio: {
          autoGainControl: false,
          echoCancellation: false,
          noiseSuppression: false,
          sampleRate: 44100,
          channelCount: 1
        }
      });

      // Web Audio API AudioContextä½œæˆ
      audioContextRef.current = new AudioContext({ sampleRate: 44100 });
      
      // MediaStreamSourceNodeä½œæˆ
      const source = audioContextRef.current.createMediaStreamSource(streamRef.current);
      
      // AnalyserNodeä½œæˆãƒ»è¨­å®š
      analyserRef.current = audioContextRef.current.createAnalyser();
      analyserRef.current.fftSize = 16384; // /test/separated-audio/æº–æ‹ 
      analyserRef.current.smoothingTimeConstant = 0.3;
      
      // æ¥ç¶š
      source.connect(analyserRef.current);
      
      // PitchDetectoråˆæœŸåŒ–ï¼ˆMcLeod Pitch Methodï¼‰
      pitchDetectorRef.current = PitchDetector.forFloat32Array(analyserRef.current.fftSize);
      pitchDetectorRef.current.clarityThreshold = 0.15; // /test/separated-audio/æº–æ‹ 

      setIsMicInitialized(true);
      console.log('[useAudioEngine] ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†');

    } catch (err) {
      handleError(`ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: ${err}`);
      throw err;
    }
  }, [handleError, isMicInitialized]);

  // ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ãƒ»ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾
  const disposeMicrophone = useCallback(() => {
    try {
      console.log('[useAudioEngine] ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾');
      
      // animationFrameåœæ­¢
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
        animationFrameRef.current = null;
      }
      
      // MediaStreamåœæ­¢
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
        streamRef.current = null;
      }
      
      // AudioContexté–‰ã˜ã‚‹
      if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
        audioContextRef.current.close();
        audioContextRef.current = null;
      }
      
      // Refãƒ»StateåˆæœŸåŒ–
      analyserRef.current = null;
      pitchDetectorRef.current = null;
      setIsMicInitialized(false);
      setCurrentPitch(null);
      setCorrectedPitch(null);
      setConfidence(0);
      
      // å€éŸ³è£œæ­£ã‚·ã‚¹ãƒ†ãƒ ã®å®Œå…¨ãƒªã‚»ãƒƒãƒˆ
      previousFrequencyRef.current = null;
      frequencyHistoryRef.current = [];
      stabilityBufferRef.current = [];
      
    } catch (err) {
      console.error('[useAudioEngine] ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾ã‚¨ãƒ©ãƒ¼:', err);
    }
  }, []);

  // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³ç¨‹æ¤œå‡ºãƒ«ãƒ¼ãƒ—
  const startPitchDetectionLoop = useCallback(() => {
    if (!audioContextRef.current || !analyserRef.current || !pitchDetectorRef.current) {
      console.error('[useAudioEngine] éŸ³ç¨‹æ¤œå‡º: å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæœªåˆæœŸåŒ–');
      return;
    }

    const analyser = analyserRef.current;
    const detector = pitchDetectorRef.current;
    const float32Array = new Float32Array(analyser.fftSize);

    const detectPitch = () => {
      if (!configRef.current.enablePitchDetection || phase !== AudioSystemPhase.SCORING_PHASE) {
        // åœæ­¢æ¡ä»¶
        animationFrameRef.current = null;
        return;
      }

      // å‘¨æ³¢æ•°ãƒ‡ãƒ¼ã‚¿å–å¾—
      analyser.getFloatTimeDomainData(float32Array);
      
      // Pitchy McLeod Pitch Methodå®Ÿè¡Œï¼ˆ/test/separated-audio/æº–æ‹ ï¼‰
      const [frequency, clarity] = detector.findPitch(float32Array, audioContextRef.current!.sampleRate);
      
      if (clarity > 0.15 && frequency > 80 && frequency < 1200) {
        // Step 1-1D: å€éŸ³è£œæ­£ã‚·ã‚¹ãƒ†ãƒ çµ±åˆå®Ÿè£…
        if (configRef.current.enableHarmonicCorrection) {
          // 1. å‹•çš„ã‚ªã‚¯ã‚¿ãƒ¼ãƒ–è£œæ­£ï¼ˆå€éŸ³èª¤æ¤œå‡ºå›é¿ï¼‰
          const correctionResult = correctHarmonicFrequency(
            frequency,
            previousFrequencyRef.current,
            harmonicConfigRef.current
          );
          
          // 2. å‘¨æ³¢æ•°å®‰å®šåŒ–ãƒãƒƒãƒ•ã‚¡å‡¦ç†
          const stabilizedFrequency = stabilizeFrequency(correctionResult.frequency);
          
          // 3. äººé–“éŸ³åŸŸãƒã‚§ãƒƒã‚¯ï¼ˆC3-C6ï¼‰
          if (stabilizedFrequency >= harmonicConfigRef.current.vocalRange.min && 
              stabilizedFrequency <= harmonicConfigRef.current.vocalRange.max) {
            
            setCurrentPitch(frequency); // å…ƒã®æ¤œå‡ºå‘¨æ³¢æ•°
            setCorrectedPitch(stabilizedFrequency); // è£œæ­£å¾Œå‘¨æ³¢æ•°
            setConfidence(clarity * correctionResult.confidence); // è£œæ­£ä¿¡é ¼åº¦ã‚’åŠ å‘³
            
            // å‰å›å‘¨æ³¢æ•°ã®æ›´æ–°
            previousFrequencyRef.current = stabilizedFrequency;
            
            console.log(`[useAudioEngine] å€éŸ³è£œæ­£: ${frequency.toFixed(1)}Hz â†’ ${stabilizedFrequency.toFixed(1)}Hz (${correctionResult.correction})`);
          } else {
            // éŸ³åŸŸå¤–ã¯ç„¡è¦–
            setCurrentPitch(null);
            setCorrectedPitch(null);
            setConfidence(0);
          }
        } else {
          // å€éŸ³è£œæ­£ç„¡åŠ¹æ™‚ã¯å¾“æ¥é€šã‚Š
          if (frequency >= 130.81 && frequency <= 1046.50) {
            setCurrentPitch(frequency);
            setConfidence(clarity);
            setCorrectedPitch(frequency); // è£œæ­£ãªã—
          } else {
            setCurrentPitch(null);
            setConfidence(0);
            setCorrectedPitch(null);
          }
        }
      } else {
        setCurrentPitch(null);
        setConfidence(0);
        setCorrectedPitch(null);
      }

      // æ¬¡ã®ãƒ•ãƒ¬ãƒ¼ãƒ äºˆç´„ï¼ˆ60fpsç›®æ¨™ï¼‰
      animationFrameRef.current = requestAnimationFrame(detectPitch);
    };

    // æ¤œå‡ºãƒ«ãƒ¼ãƒ—é–‹å§‹
    detectPitch();
  }, [phase]);

  // éŸ³æ¥½çš„å¦¥å½“æ€§è©•ä¾¡é–¢æ•°ï¼ˆ/test/separated-audio/ç§»æ¤ï¼‰
  const calculateMusicalScore = useCallback((frequency: number): number => {
    // A4=440Hzã‚’åŸºæº–ã¨ã—ãŸåŠéŸ³æ•°è¨ˆç®—
    const A4 = 440;
    const semitonesFromA4 = 12 * Math.log2(frequency / A4);
    
    // ç´”æ­£éŸ³ç¨‹ï¼ˆåŠéŸ³å˜ä½ï¼‰ã‹ã‚‰ã®ãšã‚Œã‚’è¨ˆç®—
    const nearestSemitone = Math.round(semitonesFromA4);
    const deviation = Math.abs(semitonesFromA4 - nearestSemitone);
    
    // ãšã‚ŒãŒå°ã•ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼ˆ0ï½1ã®ç¯„å›²ï¼‰
    return Math.max(0, 1 - (deviation * 4)); // 1/4åŠéŸ³ã§ã‚¹ã‚³ã‚¢0ã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´
  }, []);

  // å‹•çš„ã‚ªã‚¯ã‚¿ãƒ¼ãƒ–è£œæ­£ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆ/test/separated-audio/ç§»æ¤ãƒ»iPhoneæœ€é©åŒ–ç‰ˆï¼‰
  const correctHarmonicFrequency = useCallback((
    detectedFreq: number,
    previousFreq: number | null,
    config: HarmonicCorrectionConfig
  ): { frequency: number, confidence: number, correction: string } => {
    // iPhoneæœ€é©åŒ– - é…åˆ—å†åˆ©ç”¨ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
    const fundamentalCandidates = [
      detectedFreq,                    // ãã®ã¾ã¾ï¼ˆåŸºéŸ³ã®å¯èƒ½æ€§ï¼‰
      detectedFreq / 2.0,             // 1ã‚ªã‚¯ã‚¿ãƒ¼ãƒ–ä¸‹ï¼ˆ2å€éŸ³ â†’ åŸºéŸ³ï¼‰
      detectedFreq / 3.0,             // 3å€éŸ³ â†’ åŸºéŸ³
      detectedFreq / 4.0,             // 4å€éŸ³ â†’ åŸºéŸ³
      detectedFreq * 2.0,             // 1ã‚ªã‚¯ã‚¿ãƒ¼ãƒ–ä¸Šï¼ˆä½ãæ­Œã£ãŸå ´åˆï¼‰
    ];
    
    // iPhoneæœ€é©åŒ–: äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§è¨ˆç®—è² è·è»½æ¸›
    let bestFreq = detectedFreq;
    let bestScore = -1;
    let bestCorrectionType = 'ãªã—';
    
    for (let i = 0; i < fundamentalCandidates.length; i++) {
      const freq = fundamentalCandidates[i];
      
      // é«˜é€Ÿãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: äººé–“éŸ³åŸŸå¤–ã¯å³åº§ã«é™¤å¤–ï¼ˆå‡¦ç†è² è·è»½æ¸›ï¼‰
      if (freq < config.vocalRange.min || freq > config.vocalRange.max) {
        continue;
      }
      
      // è»½é‡åŒ–è©•ä¾¡ï¼ˆiPhoneæœ€é©åŒ–ï¼‰
      const vocalRangeScore = 1.0; // æ—¢ã«éŸ³åŸŸå†…ç¢ºèªæ¸ˆã¿
      
      const continuityScore = previousFreq 
        ? 1.0 - Math.min(Math.abs(freq - previousFreq) / previousFreq, 1.0)
        : 0.5;
      
      // éŸ³æ¥½çš„å¦¥å½“æ€§è©•ä¾¡ï¼ˆè»½é‡ç‰ˆï¼‰
      const musicalScore = calculateMusicalScore(freq);
      
      const totalScore = (vocalRangeScore * 0.4) + (continuityScore * 0.4) + (musicalScore * 0.2);
      
      // æœ€é«˜ã‚¹ã‚³ã‚¢æ›´æ–°ï¼ˆmap/reduceä¸ä½¿ç”¨ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
      if (totalScore > bestScore) {
        bestScore = totalScore;
        bestFreq = freq;
        
        // è£œæ­£ç¨®åˆ¥ã®åˆ¤å®š
        if (i === 0) bestCorrectionType = 'ãªã—';
        else if (i === 1) bestCorrectionType = '1/2å€éŸ³è£œæ­£';
        else if (i === 2) bestCorrectionType = '1/3å€éŸ³è£œæ­£';
        else if (i === 3) bestCorrectionType = '1/4å€éŸ³è£œæ­£';
        else if (i === 4) bestCorrectionType = '2å€éŸ³è£œæ­£';
      }
    }
      
    return { 
      frequency: bestFreq, 
      confidence: bestScore, 
      correction: bestCorrectionType 
    };
  }, [calculateMusicalScore]);

  // å‘¨æ³¢æ•°å®‰å®šåŒ–ãƒãƒƒãƒ•ã‚¡å‡¦ç†ï¼ˆ/test/separated-audio/ç§»æ¤ï¼‰
  const stabilizeFrequency = useCallback((frequency: number): number => {
    // å±¥æ­´ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ ï¼ˆæœ€å¤§5ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰
    stabilityBufferRef.current.push(frequency);
    if (stabilityBufferRef.current.length > 5) {
      stabilityBufferRef.current.shift(); // å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
    }
    
    // éå»5ãƒ•ãƒ¬ãƒ¼ãƒ ã®å¹³å‡ã‚’è¨ˆç®—ï¼ˆå®‰å®šåŒ–å‡¦ç†ï¼‰
    const sum = stabilityBufferRef.current.reduce((acc, freq) => acc + freq, 0);
    return sum / stabilityBufferRef.current.length;
  }, []);

  // ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚¢ãƒ³ãƒã‚¦ãƒ³ãƒˆæ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  useEffect(() => {
    return () => {
      disposeSampler();
      disposeMicrophone();
    };
  }, [disposeSampler, disposeMicrophone]);
  
  // åŸºéŸ³å†ç”Ÿæ©Ÿèƒ½ï¼ˆTone.jsçµ±åˆå®Ÿè£…ï¼‰
  const playBaseTone = useCallback(async (note: string): Promise<void> => {
    try {
      clearError();
      setPhase(AudioSystemPhase.BASE_TONE_PHASE);
      setIsPlaying(true);
      
      console.log(`[useAudioEngine] playBaseTone: ${note} (mode: ${configRef.current.mode})`);
      
      // Salamander PianoåˆæœŸåŒ–
      await initializeSampler();
      
      if (!samplerRef.current) {
        throw new Error('SampleråˆæœŸåŒ–å¤±æ•—');
      }
      
      // æ—¢å­˜3ãƒ¢ãƒ¼ãƒ‰äº’æ›ã®å†ç”Ÿãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
      console.log(`[useAudioEngine] â™ª å†ç”Ÿä¸­: ${note}`);
      samplerRef.current.triggerAttack(note, undefined, 0.8); // velocityçµ±ä¸€
      
      // å†ç”Ÿæ™‚é–“ã¯å‘¼ã³å‡ºã—å´ã§åˆ¶å¾¡ï¼ˆæ—¢å­˜ãƒ¢ãƒ¼ãƒ‰äº’æ›ï¼‰
      // ã“ã“ã§ã¯å†ç”Ÿé–‹å§‹ã®ã¿å®Ÿè¡Œ
      
    } catch (err) {
      setIsPlaying(false);
      setPhase(AudioSystemPhase.IDLE);
      handleError(`åŸºéŸ³å†ç”Ÿã‚¨ãƒ©ãƒ¼: ${err}`);
    }
  }, [clearError, handleError, initializeSampler]);
  
  // åŸºéŸ³åœæ­¢æ©Ÿèƒ½
  const stopBaseTone = useCallback(() => {
    try {
      console.log('[useAudioEngine] stopBaseTone');
      
      if (samplerRef.current) {
        // å…¨ã¦ã®éŸ³ã‚’åœæ­¢
        samplerRef.current.releaseAll();
        console.log('[useAudioEngine] ğŸ”‡ å†ç”Ÿåœæ­¢');
      }
      
      setIsPlaying(false);
      setPhase(AudioSystemPhase.IDLE);
      
    } catch (err) {
      handleError(`åŸºéŸ³åœæ­¢ã‚¨ãƒ©ãƒ¼: ${err}`);
    }
  }, [handleError]);
  
  // éŸ³ç¨‹æ¤œå‡ºé–‹å§‹ï¼ˆPitchyçµ±åˆå®Ÿè£…ï¼‰
  const startPitchDetection = useCallback(async (): Promise<void> => {
    try {
      if (!configRef.current.enablePitchDetection) {
        console.log('[useAudioEngine] éŸ³ç¨‹æ¤œå‡ºç„¡åŠ¹ï¼ˆè¨­å®šã«ã‚ˆã‚Šï¼‰');
        return;
      }
      
      clearError();
      console.log('[useAudioEngine] éŸ³ç¨‹æ¤œå‡ºé–‹å§‹');
      
      // ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³åˆæœŸåŒ–
      await initializeMicrophone();
      
      // ãƒ•ã‚§ãƒ¼ã‚ºç§»è¡Œãƒ»æ¤œå‡ºé–‹å§‹
      setPhase(AudioSystemPhase.SCORING_PHASE);
      startPitchDetectionLoop();
      
      console.log('[useAudioEngine] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³ç¨‹æ¤œå‡ºé–‹å§‹');
      
    } catch (err) {
      setPhase(AudioSystemPhase.IDLE);
      handleError(`éŸ³ç¨‹æ¤œå‡ºé–‹å§‹ã‚¨ãƒ©ãƒ¼: ${err}`);
    }
  }, [clearError, handleError, initializeMicrophone, startPitchDetectionLoop]);
  
  // éŸ³ç¨‹æ¤œå‡ºåœæ­¢
  const stopPitchDetection = useCallback(() => {
    try {
      console.log('[useAudioEngine] éŸ³ç¨‹æ¤œå‡ºåœæ­¢');
      
      // animationFrameåœæ­¢
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
        animationFrameRef.current = null;
      }
      
      // æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢
      setCurrentPitch(null);
      setCorrectedPitch(null);
      setConfidence(0);
      
      // å€éŸ³è£œæ­£ã‚·ã‚¹ãƒ†ãƒ ã®ãƒªã‚»ãƒƒãƒˆ
      previousFrequencyRef.current = null;
      stabilityBufferRef.current = [];
      
      // ãƒ•ã‚§ãƒ¼ã‚ºãƒªã‚»ãƒƒãƒˆ
      if (phase === AudioSystemPhase.SCORING_PHASE) {
        setPhase(AudioSystemPhase.IDLE);
      }
      
      // ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ - å‘¼ã³å‡ºã—å´ã§åˆ¶å¾¡å¯èƒ½ï¼‰
      // disposeMicrophone(); // å¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆè§£é™¤
      
    } catch (err) {
      handleError(`éŸ³ç¨‹æ¤œå‡ºåœæ­¢ã‚¨ãƒ©ãƒ¼: ${err}`);
    }
  }, [phase, handleError]);
  
  return {
    // åŸºéŸ³å†ç”Ÿæ©Ÿèƒ½
    playBaseTone,
    stopBaseTone,
    
    // ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³æ©Ÿèƒ½
    startPitchDetection,
    stopPitchDetection,
    
    // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡º
    currentPitch,
    correctedPitch,
    confidence,
    
    // çŠ¶æ…‹ç®¡ç†
    isPlaying,
    phase,
    error
  };
}