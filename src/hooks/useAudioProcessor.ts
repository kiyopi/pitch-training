'use client';

import { useState, useRef, useCallback, useEffect } from 'react';
import * as Tone from 'tone';

/**
 * AudioContextãƒ»éŸ³å£°å‡¦ç†åŸºç›¤ãƒ•ãƒƒã‚¯ - Step 2
 * 
 * ç›®çš„: éŸ³ç¨‹æ¤œå‡ºã«æœ€é©åŒ–ã•ã‚ŒãŸAudioContextéŸ³å£°å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
 * æ©Ÿèƒ½: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€æ—¢å­˜Tone.jsã¨ã®çµ±åˆ
 * å¯¾è±¡: Step 1ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³åŸºç›¤ã®æ‹¡å¼µ
 */

// éŸ³å£°å‡¦ç†çŠ¶æ…‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
interface AudioProcessorState {
  isProcessing: boolean;
  sampleRate: number;
  bufferSize: number;
  audioContext: AudioContext | null;
  analyserNode: AnalyserNode | null;
  error: string | null;
  isInitialized: boolean;
}

// éŸ³å£°å‡¦ç†ãƒ‡ãƒ¼ã‚¿
interface ProcessedAudioData {
  timedomainData: Float32Array | null;
  frequencyData: Uint8Array | null;
  rms: number;
  peak: number;
  timestamp: number;
}

// AudioProcessorãƒ•ãƒƒã‚¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
interface AudioProcessorHook {
  processorState: AudioProcessorState;
  startProcessing: (stream: MediaStream) => Promise<boolean>;
  stopProcessing: () => void;
  getProcessedData: () => ProcessedAudioData;
  resetError: () => void;
}

// AudioContextæœ€é©åŒ–è¨­å®š
const AUDIO_CONTEXT_CONFIG = {
  sampleRate: 44100,          // é«˜å“è³ªéŸ³ç¨‹æ¤œå‡º
  latencyHint: 'interactive' as AudioContextLatencyCategory, // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç­”
};

// AnalyserNodeè¨­å®š
const ANALYSER_CONFIG = {
  fftSize: 2048,              // å‘¨æ³¢æ•°åˆ†è§£èƒ½ï¼ˆéŸ³ç¨‹æ¤œå‡ºã«æœ€é©ï¼‰
  smoothingTimeConstant: 0.8, // ãƒã‚¤ã‚ºå¹³æ»‘åŒ–
  minDecibels: -90,           // æœ€å°ãƒ‡ã‚·ãƒ™ãƒ«
  maxDecibels: -10,           // æœ€å¤§ãƒ‡ã‚·ãƒ™ãƒ«
};

export const useAudioProcessor = (): AudioProcessorHook => {
  const [processorState, setProcessorState] = useState<AudioProcessorState>({
    isProcessing: false,
    sampleRate: 44100,
    bufferSize: 1024,
    audioContext: null,
    analyserNode: null,
    error: null,
    isInitialized: false,
  });

  // AudioContextãƒ»AnalyserNodeãƒ»MediaStreamSourceã®Ref
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserNodeRef = useRef<AnalyserNode | null>(null);
  const mediaStreamSourceRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);
  const isStoppingRef = useRef(false);

  // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ•ã‚¡
  const timedomainDataRef = useRef<Float32Array | null>(null);
  const frequencyDataRef = useRef<Uint8Array | null>(null);

  /**
   * AudioContextã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
   */
  const handleAudioContextError = (error: Error): string => {
    console.error('AudioContext error:', error);
    
    switch (error.name) {
      case 'NotAllowedError':
        return 'ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªå‡¦ç†ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚';
      case 'InvalidStateError':
        return 'AudioContextã®çŠ¶æ…‹ãŒç„¡åŠ¹ã§ã™ã€‚';
      case 'NotSupportedError':
        return 'ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã§ã¯AudioContextãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚';
      default:
        return `AudioContext ã‚¨ãƒ©ãƒ¼: ${error.message}`;
    }
  };

  /**
   * AudioContextã¨AnalyserNodeã®åˆæœŸåŒ–
   */
  const initializeAudioContext = useCallback(async (): Promise<boolean> => {
    try {
      // æ—¢å­˜ã®Tone.js AudioContextã¨ã®çµ±åˆç¢ºèª
      let audioContext: AudioContext;
      
      if (Tone.context.state === 'running') {
        // Tone.jsãŒæ—¢ã«å‹•ä½œä¸­ã®å ´åˆã¯ã€ãã®contextã‚’ä½¿ç”¨
        audioContext = Tone.context.rawContext as AudioContext;
        console.log('ğŸ”— Tone.js AudioContextçµ±åˆ');
      } else {
        // æ–°ã—ã„AudioContextã‚’ä½œæˆ
        audioContext = new AudioContext(AUDIO_CONTEXT_CONFIG);
        console.log('ğŸ†• æ–°è¦AudioContextä½œæˆ');
      }

      // AudioContextã®é–‹å§‹
      if (audioContext.state === 'suspended') {
        await audioContext.resume();
      }

      // AnalyserNodeã®ä½œæˆã¨è¨­å®š
      const analyserNode = audioContext.createAnalyser();
      analyserNode.fftSize = ANALYSER_CONFIG.fftSize;
      analyserNode.smoothingTimeConstant = ANALYSER_CONFIG.smoothingTimeConstant;
      analyserNode.minDecibels = ANALYSER_CONFIG.minDecibels;
      analyserNode.maxDecibels = ANALYSER_CONFIG.maxDecibels;

      // ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ•ã‚¡ã®åˆæœŸåŒ–
      timedomainDataRef.current = new Float32Array(analyserNode.fftSize);
      frequencyDataRef.current = new Uint8Array(analyserNode.frequencyBinCount);

      // Refã«ä¿å­˜
      audioContextRef.current = audioContext;
      analyserNodeRef.current = analyserNode;

      setProcessorState(prev => ({
        ...prev,
        audioContext: audioContext,
        analyserNode: analyserNode,
        sampleRate: audioContext.sampleRate,
        isInitialized: true,
        error: null,
      }));

      console.log('âœ… AudioContextãƒ»AnalyserNodeåˆæœŸåŒ–å®Œäº†');
      console.log('ğŸ“Š ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ:', audioContext.sampleRate);
      console.log('ğŸ“Š FFTã‚µã‚¤ã‚º:', analyserNode.fftSize);
      
      return true;
    } catch (error) {
      const errorMessage = handleAudioContextError(error as Error);
      setProcessorState(prev => ({
        ...prev,
        error: errorMessage,
        isInitialized: false,
      }));
      return false;
    }
  }, []);

  /**
   * ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ãƒ‡ãƒ¼ã‚¿å‡¦ç†
   */
  const processAudioData = useCallback(() => {
    if (!analyserNodeRef.current || !timedomainDataRef.current || !frequencyDataRef.current || isStoppingRef.current) {
      return;
    }

    try {
      const analyser = analyserNodeRef.current;
      const timedomainData = timedomainDataRef.current;
      const frequencyData = frequencyDataRef.current;

      // æ™‚é–“é ˜åŸŸãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆéŸ³ç¨‹æ¤œå‡ºç”¨ï¼‰
      analyser.getFloatTimeDomainData(timedomainData);
      
      // å‘¨æ³¢æ•°é ˜åŸŸãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆéŸ³é‡ãƒ»ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ ç”¨ï¼‰
      analyser.getByteFrequencyData(frequencyData);

      // æ¬¡ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
      animationFrameRef.current = requestAnimationFrame(processAudioData);
    } catch (error) {
      console.error('éŸ³å£°ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼:', error);
    }
  }, []);

  /**
   * éŸ³å£°å‡¦ç†é–‹å§‹
   */
  const startProcessing = useCallback(async (stream: MediaStream): Promise<boolean> => {
    try {
      // æ—¢ã«å‡¦ç†ä¸­ã®å ´åˆã¯ç„¡è¦–
      if (processorState.isProcessing || isStoppingRef.current) {
        console.log('âš ï¸ æ—¢ã«éŸ³å£°å‡¦ç†ä¸­ã¾ãŸã¯åœæ­¢å‡¦ç†ä¸­');
        return false;
      }

      // AudioContextãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆã¯åˆæœŸåŒ–
      if (!processorState.isInitialized) {
        const initialized = await initializeAudioContext();
        if (!initialized) {
          return false;
        }
      }

      if (!audioContextRef.current || !analyserNodeRef.current) {
        throw new Error('AudioContextã¾ãŸã¯AnalyserNodeãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“');
      }

      console.log('ğŸµ éŸ³å£°å‡¦ç†é–‹å§‹');

      // MediaStreamSourceã®ä½œæˆ
      const mediaStreamSource = audioContextRef.current.createMediaStreamSource(stream);
      mediaStreamSourceRef.current = mediaStreamSource;

      // MediaStreamSource â†’ AnalyserNode ã®æ¥ç¶š
      mediaStreamSource.connect(analyserNodeRef.current);

      // å‡¦ç†çŠ¶æ…‹ã®æ›´æ–°
      setProcessorState(prev => ({
        ...prev,
        isProcessing: true,
        error: null,
      }));

      // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹
      processAudioData();

      console.log('âœ… éŸ³å£°å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹å®Œäº†');
      return true;

    } catch (error) {
      const errorMessage = handleAudioContextError(error as Error);
      setProcessorState(prev => ({
        ...prev,
        error: errorMessage,
        isProcessing: false,
      }));
      console.error('âŒ éŸ³å£°å‡¦ç†é–‹å§‹å¤±æ•—:', error);
      return false;
    }
  }, [processorState.isProcessing, processorState.isInitialized, initializeAudioContext, processAudioData]);

  /**
   * éŸ³å£°å‡¦ç†åœæ­¢
   */
  const stopProcessing = useCallback(() => {
    try {
      isStoppingRef.current = true;
      console.log('ğŸ›‘ éŸ³å£°å‡¦ç†åœæ­¢é–‹å§‹');

      // ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ åœæ­¢
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
        animationFrameRef.current = null;
      }

      // MediaStreamSourceåˆ‡æ–­
      if (mediaStreamSourceRef.current) {
        mediaStreamSourceRef.current.disconnect();
        mediaStreamSourceRef.current = null;
      }

      // çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ
      setProcessorState(prev => ({
        ...prev,
        isProcessing: false,
        error: null,
      }));

      console.log('âœ… éŸ³å£°å‡¦ç†å®Œå…¨åœæ­¢');

    } catch (error) {
      console.error('âŒ éŸ³å£°å‡¦ç†åœæ­¢ã‚¨ãƒ©ãƒ¼:', error);
      
      // ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å¼·åˆ¶çš„ã«çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ
      setProcessorState(prev => ({
        ...prev,
        isProcessing: false,
        error: 'éŸ³å£°å‡¦ç†åœæ­¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚',
      }));
    } finally {
      isStoppingRef.current = false;
    }
  }, []);

  /**
   * å‡¦ç†æ¸ˆã¿éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
   */
  const getProcessedData = useCallback((): ProcessedAudioData => {
    if (!timedomainDataRef.current || !frequencyDataRef.current) {
      return {
        timedomainData: null,
        frequencyData: null,
        rms: 0,
        peak: 0,
        timestamp: Date.now(),
      };
    }

    const timedomainData = timedomainDataRef.current;
    const frequencyData = frequencyDataRef.current;

    // RMSï¼ˆRoot Mean Squareï¼‰è¨ˆç®—
    let rms = 0;
    let peak = 0;
    
    for (let i = 0; i < timedomainData.length; i++) {
      const value = Math.abs(timedomainData[i]);
      rms += value * value;
      peak = Math.max(peak, value);
    }
    
    rms = Math.sqrt(rms / timedomainData.length);

    return {
      timedomainData: new Float32Array(timedomainData),
      frequencyData: new Uint8Array(frequencyData),
      rms,
      peak,
      timestamp: Date.now(),
    };
  }, []);

  /**
   * ã‚¨ãƒ©ãƒ¼ãƒªã‚»ãƒƒãƒˆ
   */
  const resetError = useCallback(() => {
    setProcessorState(prev => ({
      ...prev,
      error: null,
    }));
  }, []);

  // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  useEffect(() => {
    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      if (mediaStreamSourceRef.current) {
        mediaStreamSourceRef.current.disconnect();
      }
      // AudioContextã¯ä»–ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã‚‚ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ã“ã“ã§ã¯closeã—ãªã„
    };
  }, []);

  return {
    processorState,
    startProcessing,
    stopProcessing,
    getProcessedData,
    resetError,
  };
};