'use client';

import { useState, useRef, useCallback } from 'react';

/**
 * ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ç®¡ç†ãƒ•ãƒƒã‚¯ - Step 1: åŸºæœ¬è¨±å¯ãƒ»éŸ³å£°å–å¾—
 * 
 * ç›®çš„: æœ€å°é™ã®ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³åˆ¶å¾¡æ©Ÿèƒ½ã‚’æä¾›
 * å¯¾è±¡: åŸºæœ¬çš„ãªON/OFFåˆ¶å¾¡ã€è¨±å¯çŠ¶æ…‹ç®¡ç†ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
 * 
 * HYBRIDè¨±å¯ã‚·ã‚¹ãƒ†ãƒ å®Œå…¨é™¤å»æ¸ˆã¿
 * iPhone Safariå¯¾å¿œæ¸ˆã¿
 * åœæ­¢ãƒœã‚¿ãƒ³æ©Ÿèƒ½çµ±åˆæ¸ˆã¿
 */

interface MicrophoneState {
  isRecording: boolean;
  error: string | null;
  permission: 'granted' | 'denied' | 'prompt';
  audioLevel: number;
  isInitialized: boolean;
  audioContext: AudioContext | null;
  analyser: AnalyserNode | null;
}

interface MicrophoneManager {
  microphoneState: MicrophoneState;
  startRecording: () => Promise<boolean>;
  stopRecording: () => void;
  resetError: () => void;
}

export const useMicrophoneManager = (): MicrophoneManager => {
  const [microphoneState, setMicrophoneState] = useState<MicrophoneState>({
    isRecording: false,
    error: null,
    permission: 'prompt',
    audioLevel: 0,
    isInitialized: false,
    audioContext: null,
    analyser: null,
  });

  const streamRef = useRef<MediaStream | null>(null);
  const isStoppingRef = useRef(false);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const animationFrameRef = useRef<number | null>(null);

  /**
   * éŸ³ç¨‹æ¤œå‡ºæœ€é©åŒ–åˆ¶ç´„
   * autoGainControlã€echoCancellationã€noiseSuppression ã‚’ç„¡åŠ¹åŒ–
   */
  const getOptimalConstraints = (): MediaStreamConstraints => ({
    audio: {
      autoGainControl: false,      // æœ€é‡è¦: è‡ªå‹•ã‚²ã‚¤ãƒ³åˆ¶å¾¡ç„¡åŠ¹
      echoCancellation: false,     // æœ€é‡è¦: ã‚¨ã‚³ãƒ¼ã‚­ãƒ£ãƒ³ã‚»ãƒ«ç„¡åŠ¹
      noiseSuppression: false,     // æœ€é‡è¦: ãƒã‚¤ã‚ºæŠ‘åˆ¶ç„¡åŠ¹
      sampleRate: 44100,           // é«˜å“è³ªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
      channelCount: 1,             // ãƒ¢ãƒãƒ©ãƒ«
    }
  });

  /**
   * éŸ³å£°ãƒ¬ãƒ™ãƒ«ç›£è¦–æ©Ÿèƒ½ï¼ˆãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—æº–æ‹ ã®é«˜ç²¾åº¦è¨ˆç®—ï¼‰
   * ãƒ†ã‚¹ãƒˆãƒšãƒ¼ã‚¸ã®å®Ÿè£…çŸ¥è¦‹ã‚’é©ç”¨
   */
  const startAudioLevelMonitoring = useCallback(() => {
    if (!analyserRef.current) return;
    
    const analyser = analyserRef.current;
    const previousVolumeRef = { current: 0 }; // éŸ³é‡ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ç”¨
    
    const updateAudioLevel = () => {
      if (!analyser || isStoppingRef.current) return;
      
      // ğŸ”Š éŸ³é‡æ¤œå‡ºç”¨ï¼š8bité…åˆ—å–å¾—ï¼ˆãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—æº–æ‹ ï¼‰
      const byteTimeDomainData = new Uint8Array(analyser.fftSize);
      analyser.getByteTimeDomainData(byteTimeDomainData);
      
      // ğŸ”Š éŸ³é‡è¨ˆç®—ï¼ˆãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—æº–æ‹ ï¼‰
      let sum = 0;
      let maxAmplitude = 0;
      
      for (let i = 0; i < byteTimeDomainData.length; i++) {
        const sample = (byteTimeDomainData[i] - 128) / 128;
        sum += sample * sample;
        maxAmplitude = Math.max(maxAmplitude, Math.abs(sample));
      }
      
      const rms = Math.sqrt(sum / byteTimeDomainData.length);
      const calculatedVolume = Math.max(rms * 200, maxAmplitude * 100);
      
      // éŸ³é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°èª¿æ•´: iPhoneå¯¾å¿œã®ãŸã‚æ„Ÿåº¦ã‚’æœ€é©åŒ– /3.5 ã«èª¿æ•´
      const rawVolumePercent = Math.min(Math.max(calculatedVolume / 3.5 * 100, 0), 100);
      
      // ãƒã‚¤ã‚ºãƒ•ãƒ­ã‚¢é™¤å»: 5%ä»¥ä¸‹ã¯ãƒã‚¤ã‚ºã¨ã—ã¦0%è¡¨ç¤º
      const noiseThreshold = 5;
      const volumePercent = rawVolumePercent > noiseThreshold ? rawVolumePercent : 0;
      
      // éŸ³é‡ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ï¼ˆã‚ˆã‚Šåå¿œã‚’è‰¯ãï¼‰
      const smoothingFactor = 0.2;
      const smoothedVolume = previousVolumeRef.current + smoothingFactor * (volumePercent - previousVolumeRef.current);
      previousVolumeRef.current = smoothedVolume;
      
      // 0-100ã®ç¯„å›²ã§stateæ›´æ–°ï¼ˆæ­£è¦åŒ–ãªã—ï¼‰
      // ãƒ†ã‚¹ãƒˆãƒšãƒ¼ã‚¸ã®å®Ÿè£…ã«åˆã‚ã›ã¦ã€ç›´æ¥ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‚’æä¾›
      setMicrophoneState(prev => ({
        ...prev,
        audioLevel: smoothedVolume
      }));
      
      animationFrameRef.current = requestAnimationFrame(updateAudioLevel);
    };
    
    updateAudioLevel();
  }, []);

  /**
   * ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
   * è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æä¾›
   */
  const handleMicrophoneError = (error: Error): string => {
    console.error('Microphone error:', error);
    
    switch (error.name) {
      case 'NotAllowedError':
        return 'ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚';
      case 'NotFoundError':
        return 'ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒã‚¤ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚';
      case 'NotReadableError':
        return 'ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ãŒä»–ã®ã‚¢ãƒ—ãƒªã§ä½¿ç”¨ä¸­ã§ã™ã€‚ä»–ã®ã‚¢ãƒ—ãƒªã‚’çµ‚äº†ã—ã¦ãã ã•ã„ã€‚';
      case 'OverconstrainedError':
        return 'ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ã®è¨­å®šã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚';
      case 'SecurityError':
        return 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åˆ¶ç´„ã«ã‚ˆã‚Šãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã€‚';
      default:
        return `ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ã‚¨ãƒ©ãƒ¼: ${error.message}`;
    }
  };

  /**
   * ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³éŒ²éŸ³é–‹å§‹
   * è¨±å¯å–å¾—ã¨éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹
   */
  const startRecording = useCallback(async (): Promise<boolean> => {
    try {
      // æ—¢ã«éŒ²éŸ³ä¸­ã®å ´åˆã¯ç„¡è¦–
      if (microphoneState.isRecording || isStoppingRef.current) {
        console.log('âš ï¸ æ—¢ã«éŒ²éŸ³ä¸­ã¾ãŸã¯åœæ­¢å‡¦ç†ä¸­');
        return false;
      }

      // ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç¢ºèª
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        setMicrophoneState(prev => ({
          ...prev,
          error: 'ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã§ã¯ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚'
        }));
        return false;
      }

      // HTTPSç¢ºèª
      if (location.protocol !== 'https:' && location.hostname !== 'localhost') {
        setMicrophoneState(prev => ({
          ...prev,
          error: 'HTTPSã¾ãŸã¯localhostã§ã®ã¿ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ãŒä½¿ç”¨ã§ãã¾ã™ã€‚'
        }));
        return false;
      }

      console.log('ğŸ™ï¸ ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³è¨±å¯è¦æ±‚é–‹å§‹');

      // æœ€é©åŒ–åˆ¶ç´„ã§ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³ã‚¢ã‚¯ã‚»ã‚¹è¦æ±‚
      const constraints = getOptimalConstraints();
      const stream = await navigator.mediaDevices.getUserMedia(constraints);

      // ã‚¹ãƒˆãƒªãƒ¼ãƒ å–å¾—æˆåŠŸ
      streamRef.current = stream;
      
      // AudioContextã¨AnalyserNodeã‚’è¨­å®š
      audioContextRef.current = new AudioContext();
      analyserRef.current = audioContextRef.current.createAnalyser();
      analyserRef.current.fftSize = 256;
      
      // éŸ³å£°ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’AnalyserNodeã«æ¥ç¶š
      const source = audioContextRef.current.createMediaStreamSource(stream);
      source.connect(analyserRef.current);
      
      setMicrophoneState(prev => ({
        ...prev,
        isRecording: true,
        isInitialized: true,
        permission: 'granted',
        error: null,
        audioLevel: 0,
        audioContext: audioContextRef.current,
        analyser: analyserRef.current,
      }));

      // éŸ³å£°ãƒ¬ãƒ™ãƒ«ç›£è¦–é–‹å§‹
      startAudioLevelMonitoring();

      console.log('âœ… ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³è¨±å¯ãƒ»éŸ³å£°å–å¾—æˆåŠŸ');
      console.log('ğŸ“Š éŸ³å£°åˆ¶ç´„:', constraints.audio);

      return true;

    } catch (error) {
      const errorMessage = handleMicrophoneError(error as Error);
      
      setMicrophoneState(prev => ({
        ...prev,
        isRecording: false,
        permission: 'denied',
        error: errorMessage,
        audioLevel: 0,
        audioContext: null,
        analyser: null,
      }));

      console.error('âŒ ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³é–‹å§‹å¤±æ•—:', error);
      return false;
    }
  }, [microphoneState.isRecording]);

  /**
   * ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³éŒ²éŸ³åœæ­¢
   * åœæ­¢ãƒœã‚¿ãƒ³æ©Ÿèƒ½çµ±åˆæ¸ˆã¿
   */
  const stopRecording = useCallback(() => {
    try {
      isStoppingRef.current = true;
      console.log('ğŸ›‘ ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³åœæ­¢é–‹å§‹');

      // ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ãƒ¬ãƒ¼ãƒ åœæ­¢
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
        animationFrameRef.current = null;
      }

      // AudioContextåœæ­¢
      if (audioContextRef.current) {
        audioContextRef.current.close();
        audioContextRef.current = null;
      }

      // AnalyserNodeåœæ­¢
      analyserRef.current = null;

      // MediaStreamåœæ­¢
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => {
          track.stop();
          track.enabled = false;  // iPhone Safariç¢ºå®Ÿåœæ­¢
        });
        streamRef.current = null;
        console.log('âœ… MediaStreamåœæ­¢å®Œäº†');
      }

      // çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ
      setMicrophoneState(prev => ({
        ...prev,
        isRecording: false,
        isInitialized: false,
        audioLevel: 0,
        error: null,
        audioContext: null,
        analyser: null,
      }));

      console.log('âœ… ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³å®Œå…¨åœæ­¢');

    } catch (error) {
      console.error('âŒ ãƒã‚¤ã‚¯ãƒ­ãƒ•ã‚©ãƒ³åœæ­¢ã‚¨ãƒ©ãƒ¼:', error);
      
      // ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å¼·åˆ¶çš„ã«ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
        animationFrameRef.current = null;
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
        audioContextRef.current = null;
      }
      analyserRef.current = null;

      // ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚å¼·åˆ¶çš„ã«çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ
      setMicrophoneState(prev => ({
        ...prev,
        isRecording: false,
        isInitialized: false,
        audioLevel: 0,
        error: 'åœæ­¢å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚',
        audioContext: null,
        analyser: null,
      }));
    } finally {
      isStoppingRef.current = false;
    }
  }, []);

  /**
   * ã‚¨ãƒ©ãƒ¼ãƒªã‚»ãƒƒãƒˆ
   * ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¨ãƒ©ãƒ¼ã‚’ç¢ºèªå¾Œã«ãƒªã‚»ãƒƒãƒˆ
   */
  const resetError = useCallback(() => {
    setMicrophoneState(prev => ({
      ...prev,
      error: null,
    }));
  }, []);

  return {
    microphoneState,
    startRecording,
    stopRecording,
    resetError,
  };
};